{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introducción a Dask\n",
    "\n",
    "Dask es una biblioteca de python diseñada para cómputo científico/analítico en gran escala. Existen dos grandes componentes de  Dask:\n",
    "\n",
    "1. **Cómputo de tareas**: Para ejecutar un _workflow_ o _pipeline_ de tareas en paralelo. Esto es similar a Airflow o Luigi, pero de \"bajo nivel\".\n",
    "2. **Grandes datos**: Datos que no quepan en memoria o no se puedan procesar cómodamente en una computadora personal. Dask provee una solución similar a Spark, pero con sintáxis de pandas/numpy. Este componente está construido sobre el anterior\n",
    "\n",
    "## ¿Por qué Dask?\n",
    "\n",
    "![](resources/growth.png)\n",
    "\n",
    "* API Familiar (parecido a numpy/pandas)\n",
    "* Python no es un extra \n",
    "* Escala fácilmente y de manera flexible\n",
    "* Desarrollo constante \n",
    "* Comunidad activa\n",
    "\n",
    "## Dask vs Spark\n",
    "\n",
    "### Lenguaje\n",
    "* Spark está escrito en Scala con soporte (secundario) para Python y R. Puede utilizar código para la JVM\n",
    "* Dask está escrito en Python y sólo soporta Python. Puede utilizar código de C/C++/Fortran a través de las integraciones con Python\n",
    "\n",
    "### Ecosistema\n",
    "* Spark es parte de un ecosistema inspirado en el (Sparkling Water, Spark ML, etc). \"Juega bonito\" con otros proyectos de Apache\n",
    "* Dask es parte del ecosistema Python. \"Juega bonito\" con otras librerías de Python como pandas, numpy, etc\n",
    "\n",
    "### Edad y Confianza\n",
    "* Spark es más viejo y maduro (2010), y por lo tanto se ha ganado un lugar en la industria\n",
    "* Dask es más joven (2014) y en desarrollo constante, pero intenta ser una extensión de otras librerías confiables como numpy/pandas\n",
    "\n",
    "### Alcance\n",
    "* Spark está diseñado para trabajar con abstracciones de alto nivel. Tareas típicas de BI y queries SQL son sus principales casos de uso\n",
    "* Dask está pensado para trabajar a un nivel más bajo. Paralelizar y ejecutar tareas arbitrarias es uno de sus objetivos, aunque también incluye algunas abstracciones de alto nivel.\n",
    "\n",
    "### Escoge Spark si...\n",
    "* Prefieres Scala o SQL\n",
    "* Necesitas operar con código basado en la JVM\n",
    "* Quieres una solución establecida y robusta para producción\n",
    "* Necesitas programar ETLs sencillos\n",
    "* Principalmente realizas tareas de BI con un poco de ML\n",
    "\n",
    "### Escoge Dask si...\n",
    "* Prefieres Python o código nativo (C/Fortran)\n",
    "* Tu caso de uso es complejo y no es fácil de manejar en Spark\n",
    "* Prefieres un proyecto más ligero que te permita realizar cómputo en paralelo\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conéctandonos al cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client\n",
    "\n",
    "client = Client()\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La manera de distribuir trabajo en Dask es similar a Spark. Tenemos un sólo _scheduler_ que se encarga de agendar las tareas y distribuirlas a los _workers_ activos. Esto nos permite escalar a cientos de CPUs.\n",
    "\n",
    "![](resources/workers.png)\n",
    "\n",
    "Dask también nos permite monitorear recursos del sistema\n",
    "\n",
    "![](resources/system.png)\n",
    "\n",
    "También nos permite monitorear la ejecución de tareas\n",
    "\n",
    "![](resources/task_stream.png)\n",
    "\n",
    "Al igual que en Spark, podemos crear un cliente dirigiéndonos simplemente a la dirección del _scheduler_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computo _Lazy_ y en paralelo\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con Dask podemos generar una \"receta\" para la ejecución de tareas, sin ejecutarla en ese momento. Esto nos permite no \"bloquear\" nuestro programa y es lo que permite la paralelización"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask import delayed\n",
    "\n",
    "# Decoramos nuestras funciones con `delayed`, para indicar que no se deben ejecutar en este momento\n",
    "\n",
    "@delayed\n",
    "def inc1(x):\n",
    "    return x + 1\n",
    "\n",
    "\n",
    "@delayed\n",
    "def mul2(x):\n",
    "    return x*2\n",
    "\n",
    "\n",
    "@delayed\n",
    "def add(x, y):\n",
    "    return x + y\n",
    "\n",
    "# Creamos la \"receta\" de ejecución\n",
    "x = inc1(15)\n",
    "y = mul2(7)\n",
    "total = add(x, y)\n",
    "total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Podemos visualizar el grafo de tareas generado por esta receta\n",
    "total.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Realizamos el cómputo de la tarea\n",
    "result = total.compute()\n",
    "print(result)\n",
    "\n",
    "# O bien, lo realizamos de manera remota \n",
    "computed = client.compute(total) # Esto no bloquea\n",
    "computed.result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sequential(s):\n",
    "    a = np.random.normal(0, 1, s)\n",
    "    b = np.random.normal(5, 3, s)\n",
    "    c = np.random.normal(7, 2, s)\n",
    "    \n",
    "    return np.percentile(a, 80) + np.percentile(b, 60) + np.percentile(c, 99)\n",
    "\n",
    "delayed_normal = delayed(np.random.normal)\n",
    "delayed_percentile = delayed(np.percentile)\n",
    "def in_parallel(s):\n",
    "    a = delayed_normal(0, 1, s)\n",
    "    b = delayed_normal(5, 3, s)\n",
    "    c = delayed_normal(7, 2, s)\n",
    "\n",
    "    return (delayed_percentile(a, 80) + delayed_percentile(b, 60) + delayed_percentile(c, 99)).compute()\n",
    "\n",
    "print(\"Sequential:\")\n",
    "%time sequential(int(1e7))\n",
    "\n",
    "print(\"Parallel:\")\n",
    "%time in_parallel(int(1e7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = [(0, 1), (5, 3), (7, 2)]\n",
    "percentiles = [80, 60, 99]\n",
    "def in_parallel(s, params, percentiles):\n",
    "    normal_samples = [delayed_normal(p[0], p[1], s) for p in params]\n",
    "    return sum([delayed_percentile(sample, p) for sample, p in zip(normal_samples, percentiles)]).compute()\n",
    "    \n",
    "%time in_parallel(int(1e7), params, percentiles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ¿Cómo cambia este resultado con el parámetro de tamaño `s`? ¿Por qué?\n",
    "* ¿Cómo cambiarías la función para usar ciclos y parámetros (de la normal y percentiles)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejercicio: escribe una función para checar si un número es primo y pruébala con los sig. numeros. \n",
    "#  Después, ejecuta el ciclo en paralelo y compara el tiempo de ejecución de ambos métodos\n",
    "import math\n",
    "nums = [7961147, 1627785787, 402938420121, 2093482093844387, \n",
    "        211236347099, 3718687359123, 871144421117, 650280751121]\n",
    "\n",
    "def is_prime(n):\n",
    "    '''Checa si n es primo o no'''\n",
    "    # Hint: sólo tienes que checar los factores hasta la raíz cuadrada de n\n",
    "    return False\n",
    "\n",
    "%time [is_prime(n) for n in nums]\n",
    "\n",
    "delayed_is_prime = delayed(is_prime)\n",
    "%time [delayed_is_prime(n).compute() for n in nums]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _Bags_\n",
    "\n",
    "Los _bags_ de Dask están diseñados para lidiar con datos semi-estructurados o no estructurados (e.g. documentos JSON, texto plano). Son análogos a los RDD de Spark y proveen una abstracción a los conceptos de `map`, `filter`, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask import bag\n",
    "\n",
    "taxis = bag.read_text(\"../data/taxis.json\")\n",
    "\n",
    "# Cada línea es un JSON en texto\n",
    "print(taxis.take(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "taxis = taxis.map(json.loads) # Lazy!\n",
    "taxis.take(1) #Sólo ejecuta un `json.loads`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter\n",
    "b_type = taxis.filter(lambda d: d[\"car_type\"] == \"B\")\n",
    "print(\"Autos tipo B: {}\".format(b_type.count().compute()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pluck\n",
    "print(\"Primeros 10 ids: {}\".format(taxis.pluck(\"taxi_id\").take(10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplasta\n",
    "trips = taxis.pluck(\"trips\").flatten()\n",
    "trips.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Algunas operaciones se pueden aplicar directamente\n",
    "(trips.pluck(\"fare_amount\")\n",
    "      .mean()\n",
    "      .compute())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Group By\n",
    "Agrupa los elementos del bag de acuerdo a la evaluación de alguna función \"llave\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = bag.from_sequence(list(range(10)))\n",
    "b.groupby(lambda x: x % 3).compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fold By\n",
    "\n",
    "Agrupa y **reduce** de acuerdo a la evaluación de alguna función. Cumple un propósito similar a `reduceByKey` o `combineByKey` de Spark.\n",
    "\n",
    "Necesita 3 cosas:\n",
    "\n",
    "1. Una función \"llave\" para agrupar los elementos\n",
    "2. Una función binaria para reducir elementos por pares\n",
    "3. Una función binaria para reducir el resultado de dos reducciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "def incr(x, y):\n",
    "    return x + 1\n",
    "\n",
    "def add(x, y):\n",
    "    return x + y\n",
    "\n",
    "print(trips.foldby(key=lambda x: x[\"tip_amount\"] > 10, # Si el tip es > 10 o no\n",
    "              binop=incr, # Incrementa 1 por obs\n",
    "              initial=0, \n",
    "              combine=add, # Suma cada cuenta\n",
    "              combine_initial=0)\n",
    "       .compute())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "(trips.groupby(grouper=lambda x: x[\"tip_amount\"] > 10, shuffle=\"disk\")\n",
    "      .starmap(lambda k, v: (k, len(v)))\n",
    "      .compute())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Por qué es tan lento `groupby` comparado con `foldby`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Ejercicio: Denormalización\n",
    "# Obtener un dataframe de trips a partir de nuestro bag de taxis SIN PERDER la información del taxi\n",
    "# Hint: Una vez que el bag contenga los elementos correctos, usar `my_bag.to_dataframe()`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _Dataframes_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El dataframe es un concepto popular en análisis de datos para manejar datos estructurados de manera tabular. Dask implementa una interface (API) de dataframes similar a la de pandas. De hecho, un dataframe de Dask está formado de varios dataframes de pandas en paralelo, y una operación en el dataframe de Dask genera una operación para cada uno de esos dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask import dataframe\n",
    "trips_df = dataframe.read_csv(\"../data/trips.csv\")\n",
    "trips_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Truco para convertir a datetime...\n",
    "trips_df.tpep_pickup_datetime = trips_df.tpep_pickup_datetime.astype('M8[us]')\n",
    "trips_df.tpep_dropoff_datetime = trips_df.tpep_dropoff_datetime.astype('M8[us]')\n",
    "trips_df.tpep_dropoff_datetime.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En general, los dataframes de pandas implementan más funciones (i.e. tienen más features) que los de Dask. Los dataframes de Dask son una manera de interactuar _a la pandas_ con datos que no caben en memoria"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicios simples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usa `describe` para obtener información básica del dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcula la propina promedio para cada tipo de coche (A, B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grafica dos boxplot para comparar la distribución de propina según el tipo de coche"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grafica un histograma de los viajes según la hora del día"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agrega una columna para la duración del viaje\n",
    "trips_df[\"trip_duration\"] = trips_df.tpep_dropoff_datetime - trips_df.tpep_pickup_datetime\n",
    "trips_df.trip_duration.map(lambda d: d.total_seconds()).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grafica un scatterplot de duración de viaje vs. distancia de viaje\n",
    "\n",
    "# Para un scatterplot de verdad la única manera es traer los datos completos, o usar un sample\n",
    "print(len(trips_df))\n",
    "local_sample = trips_df.sample(0.1).compute()\n",
    "\n",
    "plt.scatter(local_sample.trip_duration.map(lambda d: d.total_seconds()), local_sample.trip_distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ¿Cuál es la proporción de propina promedio para los viajes con más de 3 pasajeros?\n",
    "trips_df[trips_df.passenger_count > 3].tip_amount.mean().compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indices\n",
    "\n",
    "Dada la naturaleza en paralelo de Dask, los índices cobran mucho mayor relevancia. Todas las operaciones que puedan hacerse sobre un índice serán mucho más rápidas, pero cambiar de índice será costoso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trips_indexed = trips_df.set_index(\"tpep_pickup_datetime\") # Muy costoso en datos grandes, evitar!\n",
    "trips_indexed.divisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "trips_indexed.loc[pd.Timestamp('2015-01-01 01:08:55')].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio/Tarea\n",
    "\n",
    "Aprovecha la capacidad de Dask para realizar cómputo en paralelo para ajustar un modelo para predecir la proporción de propina de un viaje. Realiza búsqueda de hiperparámetros en grid con cross validation. Puedes usar funciones de scikit learn. Recuerda usar el decorador `delayed` para ejecutar en paralelo.\n",
    "\n",
    "* ¿Qué tan rápido es buscar en paralelo comparado con una búsqueda secuencial en python?\n",
    "\n",
    "Haz lo mismo que arriba, pero utilizando la biblioteca Dask-ML http://dask-ml.readthedocs.io/en/latest/ \n",
    "\n",
    "* ¿Cómo se comparan los tiempos de ejecución de tu búsqueda con la de Dask ML?\n",
    "\n",
    "**Bonus**\n",
    "\n",
    "Haz lo mismo utilizando Spark ML\n",
    "\n",
    "* ¿Cómo se comparan los tiempos de ejecución de Spark vs Dask?\n",
    "\n",
    "Usa los datos en s3://dask-data/nyc-taxi/2015/yellow_tripdata_2015-01.csv\n",
    "\n",
    "* ¿Cambia alguno de los resultados anteriores?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
